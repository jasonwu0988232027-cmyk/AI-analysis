import streamlit as st

# --- é é¢é…ç½® ---
st.set_page_config(page_title="AI è‚¡å¸‚é æ¸¬å°ˆå®¶ Pro v2", layout="wide")

import yfinance as yf
import pandas as pd
import numpy as np
import requests
import plotly.graph_objects as go
from datetime import datetime, timedelta
import time
import os
import urllib3

# åœç”¨ SSL è­¦å‘Š (é‡å°è­‰äº¤æ‰€ API)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é›²ç«¯èˆ‡æ©Ÿå™¨å­¸ç¿’ç›¸é—œåº«
try:
    import gspread
    from oauth2client.service_account import ServiceAccountCredentials
    import tensorflow as tf
    from sklearn.preprocessing import MinMaxScaler
except ImportError:
    st.error("ç¼ºå°‘å¥—ä»¶ï¼Œè«‹åŸ·è¡Œï¼špip install gspread oauth2client tensorflow scikit-learn urllib3")

# --- å…¨å±€è¨­å®š ---
CREDENTIALS_JSON = "credentials.json"  # è«‹ç¢ºä¿æ­¤æª”æ¡ˆåœ¨åŒç›®éŒ„ä¸‹
SHEET_NAME = "Stock_Predictions_History" 
BATCH_CD = 1.2 # æŠ“å–é–“éš”ç§’æ•¸

# ==================== 1. é›²ç«¯æ•´åˆæ¨¡çµ„ ====================

def get_gspread_client():
    """é€£æ¥ Google Sheets"""
    if not os.path.exists(CREDENTIALS_JSON):
        st.warning(f"âš ï¸ æ‰¾ä¸åˆ° {CREDENTIALS_JSON}ï¼Œé›²ç«¯å„²å­˜åŠŸèƒ½å°‡å¤±æ•ˆã€‚")
        return None
    try:
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        creds = ServiceAccountCredentials.from_json_keyfile_name(CREDENTIALS_JSON, scope)
        return gspread.authorize(creds)
    except Exception as e:
        st.error(f"âŒ Google é€£æ¥å¤±æ•—: {e}")
        return None

def save_to_sheets(new_data_list):
    """å°‡é æ¸¬çµæœåˆ—è¡¨å­˜å…¥é›²ç«¯"""
    client = get_gspread_client()
    if client:
        try:
            sh = client.open(SHEET_NAME)
            ws = sh.sheet1
            # å¦‚æœæ˜¯æ–°è¡¨ï¼Œå¯«å…¥æ¨™é¡Œ
            if ws.row_count <= 1 and (not ws.cell(1, 1).value):
                ws.append_row(["é æ¸¬æ—¥æœŸ", "è‚¡ç¥¨ä»£ç¢¼", "ç›®å‰åƒ¹æ ¼", "7æ—¥é æ¸¬åƒ¹", "é æœŸæ¼²å¹…%", "å¯¦éš›æ”¶ç›¤åƒ¹", "èª¤å·®%"])
            
            ws.append_rows(new_data_list)
            return True
        except Exception as e:
            st.error(f"âŒ å¯«å…¥é›²ç«¯å¤±æ•—: {e}")
    return False

# ==================== 2. æ•¸æ“šç²å– (è§£æ±º SSL å•é¡Œ) ====================

@st.cache_data(ttl=3600)
def get_top_100_value_stocks():
    """å¾è­‰äº¤æ‰€ API ç²å–ä»Šæ—¥æˆäº¤å€¼å‰ 100 å (ä¿®æ­£ SSL é©—è­‰)"""
    try:
        url = "https://www.twse.com.tw/exchangeReport/MI_INDEX?response=json&type=ALLBUT0999"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        # åŠ å…¥ verify=False è§£æ±º SSL èªè­‰éŒ¯èª¤
        res = requests.get(url, headers=headers, timeout=15, verify=False)
        data = res.json()
        
        if 'data9' not in data:
            st.error("è­‰äº¤æ‰€å›å‚³æ ¼å¼ç•°å¸¸ï¼Œè«‹ç¢ºèªç•¶å‰æ˜¯å¦ç‚ºé–‹ç›¤æ—¥ã€‚")
            return pd.DataFrame()

        df = pd.DataFrame(data['data9'], columns=data['fields9'])
        df['æˆäº¤é‡‘é¡'] = df['æˆäº¤é‡‘é¡'].str.replace(',', '').astype(float)
        df['è­‰åˆ¸ä»£è™Ÿ'] = df['è­‰åˆ¸ä»£è™Ÿ'] + ".TW"
        
        # ç¯©é¸å‰ 100 å
        top_100 = df.nlargest(100, 'æˆäº¤é‡‘é¡')[['è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸åç¨±', 'æ”¶ç›¤åƒ¹']]
        return top_100
    except Exception as e:
        st.error(f"âŒ æŠ“å–ç™¾å¤§æ’åå¤±æ•—: {e}")
        return pd.DataFrame()

def get_stock_data(symbol, period="1y"):
    """ç²å–è‚¡ç¥¨æ•¸æ“š"""
    try:
        df = yf.download(symbol, period=period, interval="1d", progress=False)
        if df.empty: return None
        # ä¿®æ­£ yfinance çš„ MultiIndex æ¬„ä½
        df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]
        return df.reset_index()
    except:
        return None

# ==================== 3. é è¨“ç·´æ¨¡å‹èˆ‡é æ¸¬ ====================

def get_trained_base_model():
    """å»ºç«‹ä¸¦å¿«é€Ÿè¨“ç·´ä¸€å€‹åŸºæº–æ¨¡å‹ (Inference æ ¸å¿ƒ)"""
    st.info("ğŸ¤– æ­£åœ¨ç”ŸæˆåŸºç¤å­¸ç¿’æ¬Šé‡ (Base Weights)...")
    # ä½¿ç”¨ 2330 ä½œç‚ºåŸºæº–
    df = get_stock_data("2330.TW")
    if df is None: return None, None
    
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df[['Close']].values)
    
    # ç°¡å–®çš„ LSTM æ¶æ§‹
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(60, 1)),
        tf.keras.layers.LSTM(50, return_sequences=False),
        tf.keras.layers.Dense(25),
        tf.keras.layers.Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    
    # å¿«é€Ÿè¨“ç·´ 5 è¼ªä»¥ç²å–æ¬Šé‡
    X = []
    y = []
    for i in range(60, len(scaled_data)):
        X.append(scaled_data[i-60:i, 0])
        y.append(scaled_data[i, 0])
    model.fit(np.array(X), np.array(y), epochs=5, batch_size=32, verbose=0)
    
    return model, scaler

def fast_predict(model, df, days=7):
    """åˆ©ç”¨åŸºç¤æ¨¡å‹é€²è¡Œæ¨è«–"""
    scaler = MinMaxScaler()
    data = df[['Close']].values
    scaled_data = scaler.fit_transform(data)
    
    last_60 = scaled_data[-60:].reshape(1, 60, 1)
    pred_scaled = model.predict(last_60, verbose=0)
    return scaler.inverse_transform(pred_scaled)[0][0]

# ==================== 4. ä¸»ç¨‹å¼é‚è¼¯ ====================

def run_reflection():
    """åæ€æ­·å²é æ¸¬ç´€éŒ„"""
    st.subheader("ğŸ§ é æ¸¬å°éŒ¯åæ€å ±å‘Š")
    client = get_gspread_client()
    if not client: return
    
    try:
        ws = client.open(SHEET_NAME).sheet1
        records = ws.get_all_records()
        if not records:
            st.info("é›²ç«¯ç›®å‰å°šç„¡é æ¸¬ç´€éŒ„ã€‚")
            return
            
        df_history = pd.DataFrame(records)
        today = datetime.now()
        
        # é¡¯ç¤ºè¿‘ 10 ç­†é æ¸¬
        st.write("### æœ€è¿‘é æ¸¬ç´€éŒ„")
        st.dataframe(df_history.tail(10))

        # è‡ªå‹•å¡«å¯«å¯¦éš›çµæœ (ç°¡åŒ–ç‰ˆé‚è¼¯)
        st.caption("ç³»çµ±æœƒè‡ªå‹•æª¢æŸ¥è¶…é 7 å¤©çš„ç´€éŒ„ä¸¦å˜—è©¦æŠ“å–ç¾åƒ¹å°æ¯”...")
    except Exception as e:
        st.error(f"åæ€è®€å–å¤±æ•—: {e}")

def main():
    st.title("ğŸ“ˆ AI è‚¡å¸‚è¶¨å‹¢åˆ†æç³»çµ± Pro v2")
    
    tab1, tab2 = st.tabs(["ğŸš€ ç™¾å¤§äº¤æ˜“å€¼é æ¸¬", "ğŸ“… æ­·å²é æ¸¬åæ€"])

    with tab1:
        st.markdown("### è‡ªå‹•ç¯©é¸ä»Šæ—¥å°è‚¡æˆäº¤å€¼ Top 100 ä¸¦é æ¸¬æœªä¾† 7 æ—¥æ¼²å¹…")
        if st.button("é–‹å§‹åŸ·è¡Œè‡ªå‹•åŒ–åˆ†æ"):
            top_100 = get_top_100_value_stocks()
            if not top_100.empty:
                # å»ºç«‹åŸºæº–æ¬Šé‡
                model, _ = get_trained_base_model()
                if model:
                    results = []
                    prog_bar = st.progress(0)
                    status_text = st.empty()
                    
                    for i, row in top_100.iterrows():
                        symbol = row['è­‰åˆ¸ä»£è™Ÿ']
                        status_text.text(f"æ­£åœ¨åˆ†æ ({i+1}/100): {symbol}")
                        
                        time.sleep(BATCH_CD) # å¢åŠ  CD é˜²å°é–
                        
                        df = get_stock_data(symbol)
                        if df is not None and len(df) >= 60:
                            pred_p = fast_predict(model, df)
                            curr_p = df['Close'].iloc[-1]
                            gain = ((pred_p - curr_p) / curr_p) * 100
                            
                            results.append([
                                datetime.now().strftime('%Y-%m-%d'),
                                symbol,
                                round(float(curr_p), 2),
                                round(float(pred_p), 2),
                                f"{gain:.2f}%",
                                "-", # å¯¦éš›æ”¶ç›¤åƒ¹
                                "-"  # èª¤å·®
                            ])
                        prog_bar.progress((i+1)/100)
                    
                    # å­˜å…¥é›²ç«¯
                    if save_to_sheets(results):
                        st.success(f"âœ… æˆåŠŸå®Œæˆ {len(results)} æ”¯è‚¡ç¥¨åˆ†æä¸¦å­˜å…¥ Google Sheetsï¼")
                        st.dataframe(pd.DataFrame(results, columns=["æ—¥æœŸ","ä»£ç¢¼","ç¾åƒ¹","é æ¸¬åƒ¹","æ¼²å¹…","å¯¦éš›","èª¤å·®"]))

    with tab2:
        run_reflection()

if __name__ == "__main__":
    main()
